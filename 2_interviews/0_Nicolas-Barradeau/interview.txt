Title: Nicolas Barradeau

----

Date: 2022-02-15

----

Language: FR

----

Audio: head-irad-dml-interview-nicolas-barradeau.m4a

----

Interviewee:

[Nicolas Barradeau](https://www.barradeau.com)

----

Interviewer:

[Alexia Mathieu](https://www.hesge.ch/head/annuaire/alexia-mathieu)

----

Text:

The interview was conducted via Zoom.

----

Transcript:

**Alexia:** Bonjour Nicolas, merci de participer à notre projet de recherche. Donc tu travailles en tant que freelance et creative coder tu es spécialisé dans le graphisme en ligne 2D, 3D trois D en temps réel, les animations et la data visualisation et tu organisesdepuis plusieurs années le festival de creative coding GROW et puis la raison pour laquelle on vous t'interviewé dans le cadre de ce projet, c'était par rapport à comment tu utilises le machine learning dans ta pratique. Et du coup, ma première question, ça serait qu'est ce qu'il t'a amené à expérimenter avec du machine learning en particulier ?

**Nicolas:** J'y serais jamais allé tout seul alors que j'ai été forcé . C'était en deux mille seize et j'ai commencé à bosser pour le GAC qui est le Google Arts and Culture Lab, ils avaient besoin d'un coup de main sur une Data visualisation justement en 3D et c'était des petites images qui veulent dans l'espace donc ça c'était pas du tout nouveau en fait quelque chose qu'on voyait beaucoup des particules qui volent dans l'espace, qui font des qui font des formes . Et la différence c'est que eux étaient avaient utilisé du machine learning pour créer les formes en question. Dans le boulot avant c'était souvent des formes géométriques. Et là ils avaient une distribution qui est assez étonnant où les les images avaient été rassemblées par similarités et donc il faut du machine learning j'ai été un petit peu obligée de m'y mettre à ce moment là. Comme je venais vraiment renfort j'ai pas eu besoin de faire du Python, d'écrire du machine learning du Mais voilà, c'était mon premier contact avec ça et ça m'a bien intéressée à ce moment là, il y avait aussi un mec qui était qui était en résidence, qui s'appelle Mario Klingemann qui lui utilisait les mêmes images avec aussi du machine learning pour faire quelque chose de très différent. À cette époque là. Du coup je me suis renseigné mais c'était plus par curiosité intellectuelle et puis , mes premières vraie lignes de python ça été deux mille dix huit , j'ai vraiment commencé à mettre les mains dedans c'est un peu déstabilisant comme expérience et c'est vraiment très complexe. Il y a en particulier beaucoup de de choses qui n'arrête pas de bouger. C'est marrant parce que vu de l'extérieur, ça a l'air d'être un peu une discipline scientifique. Bien établi, bien poser tout ça alors qu'en fait c'est un c'est un chaos sans nom quoi il y a les librairies marche plus deux ou trois semaines après qu'elle soit sortie c'est qu'il faut toujours mettre à jour toujours il faut toujours tenir petit truc ensemble pour que ça marche.

**Alexia:** C'est ce que tu veux dire. Il y a plein de choses qui bougent tout le temps. C'est justement ces librairies qui fonctionnent plus ou il y a d'autres éléments qui sont. 

**Nicolas:** Il y a deux choses. Effectivement, les librairies on va dire que tous les six mois, il y a une espèce de roulement où il faut mettre à jour des modèles. Il faut mettre à jour des librairies Python. En fait, le problème, c'est Python qu'on sait que tous les scientifiques ont décidé d'utiliser Python. Et Python c'est très organique très organique. En fait, il y a un écosystème qui très très vivace, très vivant. Il y a beaucoup de librairies, donc en gros, ça permet de résoudre. beaucoup de problèmes relativement facilement. Sauf que Python lui même, le langage n'arrête pas de bouger. Il arrête pas d'évoluer. Il y a toujours des nouvelles versions de la librairie. Et donc quand les gens commencent à travailler du machine learning , ils arrivent à un moment sur une ligne du temps et prennent la version qui veulent python à ce moment là. et sauf que six mois, un an, deux ans après ça a changé X raisons soit les librairies aussi de ça devient un peu spécifique pour un début de conversation, mais en gros pour faire du il faut et que ça prenne pas trois plombes il faut soit un énorme ordinateur, soit utiliser des cartes graphiques donc en fait ça aussi c'est un des morceaux qui qu'il faut tenir toujours ensemble avec le reste pour interfacer du python avec un GPU Le plus simple c'est d'avoir des GPU nvidia déjà et d'utiliser un truc qui s'appelle cuda qui permet de faire des calculs, de déporter les calculs sur un GPU Et puis utiliser en plus une surcouche qui s'appelle cudnn pour lui faire faire spécifiquement calcul avec python voilà et donc ce que ça fait déjà on a Python à maintenir d'un côté des librairies qui on a cuda à maintenir d'un autre côté cuda évolue de son côté aussi. Donc tu certaines fonctionnalités sont plus disponibles, sont plus exposés de la même façon dans cuda Et donc il y a des librairies entre python et cuda qui doit être mis à jour aussi en permanence pour pouvoir être juste, simplement continuer de marcher. Enfin pas s'améliorer mais juste continuer de marcher. Voilà, je pensais pas que ça viendrait aussi techniques possibles. Mais 

**Alexia:** non, mais c'est pas grave, on voit là où la discussion nous mène, mais du coup peut être ramené à quelque chose de moins technique. En même temps, c'est intéressant de voilà de quoi tu pointes, ces barrières, j'avais une question que je voulais poser, est ce que toi, tu as déjà utilisé des outils , comme par exemple Runway ML enfin, ces outils qui sont là pour aider les designers. Pas forcément des connaissances de Python de programmation à expérimenter avec du machine learning du coup, je voulais savoir s'il y avait d'autres outils que tu avais utilisé. Et puis, dans un second temps, ton opinion sur ces outils là, si tu voyais des limites ou au contraire des choses. positives qui fonctionnait bien, selon 

**Nicolas:** elle. ça fait de questions, beaucoup de trucs 

**Alexia:** Je fais la différence entre des des outils comme Adobe Sensei qui sont vraiment dans des questions d'optimisation, presque d'automatisation, de tâche que les designers font tous les jours par , par exemple tu as Font Joy qui t'aide à faire des fonts pairings et en même temps t'as des outils Runway ML plus dans l'expérimentation. Du coup, moi ça m'intéresse plus de t'entendre parler sur ces ces questions d'expérimentation. Plutôt juste des choses d'automatisation 

**Nicolas:** en fait, et c'est bien de c'est bien de dissocier les deux, Runway ML donc déjà, quand j'ai commencé ma longue carrière de machine learning scientist ça n'existait pas. Et , en fait, ce qu'il avait à la place, c'était un mec qui s'appelle Gene Kogan qui était un créatif et qui lui avait commencé à faire toute une collection de sketchs, qui maintenait un repo qui s'appelait ML4Art qui était en fait plus un guide de qui m'a beaucoup servi quand j'ai commencé parce que c'était des petits . c'est l'équivalent d'un sketchen Processing en gros c'est une petite démo de quelque chose, ça c'était c'était assez bien pour commencer en tout cas pour pour un dev, parce que ça permettait à la fois de mettre les mains dedans un peu tout à la fois. C'est-à-dire le python comment le code? Je connaissais pas donc le le code c'est un peu un peu spécial et s'est structuré bizarrement bref c'est spécifique. donc de mettre à la fois la main dans le code gagné en compréhension des concepts fondamentaux du machine learning et en plus arriver à faire deux trois trucs rigolos tels que classifier les chiffres des trucs super lol et donc ça a été mon point d'entrée en fait là-dedans et je pense que la plupart des dev créatifs vont plutôt vouloir avoir cette approche en fait qui est de se frotter directement aux codes et si possible avec un code qui est mis à leur niveau quoi?

Tu vois qui est pas genre tout de suite des trucs super bourrin? Voilà sur ces entrefaites arrive Runway ML et qui est donc un outil qui met à distance justement le code et qui expose seulement les fonctionnalités de haut niveau genre appuie sur un bouton pour transformer et faire un style transfert ou explorer un style GAN des trucs comme ça et et alors ça je sais pas moi j'ai juste par curiosité voir comment il avait fait mais je trouvais que c'était assez bien gaulée et sauf que La limite que j'ai trouvé à ça, c'est que on pouvait pas entraîner un modèle et c'est un peu le nerf de la guerre.

Alors Runway ils ont maintenant complètement oublié ça. Ils sont en train de se spécialiser sur la vidéo en fait. , ils ont ils ont embauché un mec qui s'appelle Patricio Gonzalez Vivo qui est vraiment sur cette couche la comment on fait du temps réel, donc du traitement très rapides sur des images qui sont des séquences d'images. donc c'est C'est devenu très spécifique en fait Runway. Mais à l'époque, en fait les outils qui proposait était moi y en avait que quelqu'un qui m'intéresse. Et donc il y avait notamment explorer un modèle qui s'appelle Style Gan , qui est un modèle qui a été entraîné sur des visages.

Donc il y a quelque chose qui permet de créer des visages qui sont photoréalistes sont un peu state of the art dans cette catégorie là, c'est ce qui alimente le site. This person does not exist En gros, ça marche avec une séquence de chiffres qui est une espèce de un vecteur de cinq cent douze chiffres.

Et à partir de là, en fait, . Chaque chiffre est une c'est une valeur sûre dimensions. Et donc quand on peut faire varier le résultat dans de beaucoup de manières. Et en fait, Style Gan permettait d'exposer toutes ces toutes ces variations, en fait toutes les variations du modèle et Et ça, ça m'intéresse bien. Ça marche et ça marchait très bien. l'inférence donc le fait de convertir ce vecteur de chiffres en images se faisait sur un server. Donc , c'était très lent, en fait c'était très lent. Et puis ça pouvait devenir assez vite coûteux parce que faire tourner un serveur comme ça, ça coûte cher et donc voilà un moment on avait un peu de crédit gratos donc en terme de créativité, ça servait surtout à à l'exploration. Ça ne permettait pas de faire de la prospective ou de faire quelque chose qui est de de la création assistée par ordinateur. C'était vraiment un outil qui permettait de mettre à plat la richesse d'un modèle. Voilà enfin qui étaient des choses et la voilà Runway ce qui mettait à dispot. C'était simplement les commandes simplifiées sur des modèles et c'était assez cool Par contre je n'ai pas vu beaucoup de copains même si je pense avoir essayer de convertir de trois amis crear en leur disant installé ça vous allez voir c'est rigolo mais je pense pas que ce soit évident de trouver comment ajouter sa sa boîte à outils quand on est un créatif.

**Alexia:** Et c'est vrai que nous Runway, on enseigne aussi GPT-3 je trouve qu'en cinq jours ça peut faire quelque chose d'assez intéressant en tout cas mais c'est vrai qu'en effet, on n'est pas dans quelque chose de prospectif. Et ça c'est c'est intéressant que tu soulèves est est ce pourquoi selon toi les créatifs pourrait pas s'emparer que ce soit Runway ou d'autres outils ?

**Nicolas:** En fait le ML il est rentré par la porte de derrière dans Adobe quand tu fais une sélection intelligente avec des énormes, mais maintenant il y a un outil comme ça. et qui permet d'isoler une forme sur un fond donc du detrourage c'est du machine learning en fait derrière c'est c'est un modèle qui fait de la détection de saillance et qui derrière utilisent le résultat de ça pour créer un masque d'écrêtage et mais ça c'est pas écrit dessus mais on est et donc du coup il y a beaucoup d'outils comme ça qui sont pas aussi évidemment que ceux que tu evoquais, que je connais pas d'ailleurs matcher deux fonts, je sais pas quoi ils écrivent en gros dessus ML pour attirer le chaland. Mais depuis plusieurs annéesLe ML est r entrer dans les outils en fait, dont les créatifs se servent après ce qu'ils s'en servent vraiment. Moi je vais essayer. tout à l'heure, pas plus tard que aujourd'hui, pour la première fois, le détourage intelligent Et , c'est vrai que ça marche super bien.

Enfin voilà, tu avais aussi. , c'était pas vraiment du Machine Learning mais des, manières de détirer ou de de redimensionner intelligemment une image et c'est pas du machine learning mais c'est cette classe d'outils qui sont qui sont assez pointue et que normalement des créatifs pas dev ne devrait pas pouvoir utiliser 

mais sauf que des outils comme Photoshop en l'occurrence les intègre à leur suite. De façon très enfin transparent pour l'utilisateur, juste avec leur pop-up de regarder cette nouvelle feature et puis c'est tout. Et après que les que les créations ne s'en empare, je je serais incapable de dire mais il y a beaucoup de choses comme ça qui sont qui sont un peu masqué. Donc en fait, qu'ils le veuillent ou non, ils se servent déjà du machine learning. Et puis après, pour revenir à Runway en fait, voilà le seul truc où je me suis dit C'est quoi la limite de ce truc là? Parce que c'est vachement bien fait, mais c'est ce que disait quand tu ne sais pas si c'était le built-in le scrapping, mais par exemple un skill qui va manquer aux créatif, c'est de fabriquer une collection d'images pour ré-entraîner un modèle. Et c'est là que ça deviendrait vraiment , très intéressant pour eux, parce que ils auraient la main, sur tout le processus créatif. Je pense que tant que tu en es à milker un style gan ou modèle existant. C'est bien, c'est fascinant c'est tout ce que tu veux, c'est l'objet tu vois ArtBreeder.Com, son son angle à lui, c'est de dire on va vraiment mettre ça à portée de tous. Et c'est et c'est génial. Enfin en Pour le coup, il faut avoir la patience d'un créatif mais t'as une serendipité comme on dit. , une propension à avoir des accidents qui est complètement folle quoi. Dans ce modèle là, il y en a un pour les portraits, il y en a pour les paysages c'était très très vaste quoi quand on a le temps de se pencher dessus. Mais à mon avis, ça remplace quand même pas la capacité à faire un modèle. Je sais pas si tu veux faire collection de pierres précieuses par exemple. Tu prends mille image de pierres précieuses dessus tu réentraines un style Gan Et puis c'est parti mon kiki la tente tant génère des dizaines de milliers si bon, c'est un besoin qui est spécifique. Pour moi, c'est ça la limite de Runway et des outils comme ça, c'est en fait Il est d'ailleurs financièrement aucun intérêt à mettre à dispo. Un moyen de d'entrainer ton propre modèle sinon tu t'es complètement autonome après

**Alexia:** C'est aussi comme un d'acte curatorial de créer ton propre , ta propre base ce qui est passionnant c'est de voir comment ta base de données va impacter les résultats et puis faire ce processus itératif mais qu'en effet pour l'instant on retrouve pas vraiment dans les outils qui sont présents qui sont accessibles, designer . Est-ce que ton rapport aux outils de technologie numérique. C'est un moyen d'atteindre une fin dans tes travaux, alors au contraire, ça joue un rôle plus important dans ton dans ton processus,

**Nicolas:** Moi, j'ai déjà, je suis un geek donc tout ce qui est de la technologie, ça me plaît. Et , je n'ai aucun mal à les faire de la veille parce que je trouve que tout est fascinant et je suis toujours fasciné par l'évolution des outils. Et et donc je suis ça de près que ce soit du machine learning ou du javascript mais bon, soyons fous mais aussi les lib Python comme le disait tout à l'heure on est obligé de se tenir à jour donc on est obligé de regarder d'avoir les yeux un peu partout ces outils là en fait.

Alors après spécifiquement le machine learning c'est un petit peu à part pour ce qui est de tu. des allers retours ne l'un des l'influence que peut avoir l'outil sur ma pratique. Donc quand je fais des sites, quand je travaille pour de l'argent, je fais souvent des sites du coup, quasiment exclusivement des sites.

l'outil principal c'est WebGL ça relativement pas bougé depuis dix ans quoi. c'est hyper stable et donc il y a pas énormément de choses. Ou quand je vois une nouvelle feature , ça va pas influencer la manière dont je travaille, c'est assez nul. En revanche, et là le machine learning c'est assez dingo pour ça, c'est comme il y a de la recherche qui part dans toutes les directions, il y a très souvent des moments où quand je suis quelques comptes notamment AK et qui est on va dire spécialisé dans la recherche de papier improbables. sur le machine learning appliqué au graphisme chaque fois que qu'il ou elle je pense que c'est une femme, sort un papier déniche un papier, je regarde Et là, ça me donne beaucoup d'idées sur , qu'est ce qu'on pourrait faire avec ce truc là? Parce que ça va aller faire de l'estimation pose, mais en même temps régénérer un nuage de points. Et puis le papier suivant, ça va être les mêmes un an plus tard, qui ont réussi à posé un squelette au milieu de ce truc-là et qui arrive à lui à le à le faire bouger.

Ça peut être , le dernier qu'elle a posté la géniale, c'est un modèle qui permet de de stylisé, l'algo est capable de refaire un dessin auprès avec divers degrés de stylisation. En fait c'est-à-dire de il peut utiliser cinquante traits et ça va être un dessin relativement réaliste ou bien un seul trait et ça va être très stylisé et en même temps sa garde le côté sémantique en fait. Donc ça, il y a une première phase de segmentation, donc on sait que ça marche assez bien. On sait pas si c'est il y a une espèce de tu vois que s'est isolé de l'arrière plan et tu vois qu'il y a un sujet qui est vraiment détectée est et ça, c'est fascinant. Je veux dire c'est complètement fascinant. J'ai qu'une envie, c'est d'essayer de voir comment on peut le pousser à faire des trucs qui n'est pas prêt à faire et voilà, c'est peut être ça la limite, c'est enfin dans ma pratique.

Ce qui est bien quand on est dev, c'est que du coup on peut aller pousser un truc comme ça un peu plus loin, ce qui était pas nécessairement le cas dans Runway en fait et ou quand on ne va pas chercher plus loin, comment ça marche et qu'est ce qu'on pourrait en faire? Et pour moi, ça c'est quand même quelque chose pour un créatif, c'est hyper frustrant. C'est hyper frustrant de se dire bon, il est sympa ce modèle qui fait de la stylisation, de très fouillé à très simple. Mais , voilà quoi, c'est c'est un truc que je mets deux minutes à faire la même chose à la main et et puis voilà, ça a du sens. Si t'as des traitements par l'eau, en fait des trucs comme ça et tant que tu peux pas avoir de d'incidence sur comment s'est fait ce que tu disais tout à l'heure le la base de données, les inputs ça change le résultat ça fait un peu gadget quoi tu vois c'est un ça revient à la limite de Runway c'est-à-dire les modèles près tout fait et comment? quoi on explore, on explore, ça peut servir. Tu vois, je veux dire, c'est pas suffire pour pour un projet, Ça peut être ça. L'objet du projet, c'est de puiser la variété de modèles, lui faire dessiner des choses qui complètement débile ou de réinterpréter tu vois ça 

**Alexia:** Tu commençais un peu touché justement, mais la question que je voulais poser c'était selon toi, quel type d'application d'apprentissage automatique de machines en ligne dans le domaine. Du coup, beaucoup plus du design graphique sont sous utilisés ou pas. Vraiment exploré ça? Qu'est ce que toi? Tu serais vraiment curieux d'essayer Parce que c'est vrai que on a parlé des des outils qui automatise les tâches. Mais ce qui se passe des choses que toi, vraiment tu te sens, tu dis ça serait vraiment génial, où je serai curieux d'essayer

**Nicolas:** ouais, c'est un petit peu biaisé parce que c'est c'est des copains. Mais il y a des gens qui font un truc qui s'appelle Clip Drop Clip Drop au début, c'est un mec qui s'appelle Cyril Diagne qui utilise son mobile, il fait une photo. d'un objet sur un arrière plan neutre. Il appuie après sur son téléphone pour faire. Voilà ça process, donc ça enlève l'arrière-plan automatiquement. donc ça c'est du machine learning. Et ensuite il pointe sa vers une télé et en fait l'image qu'il a détourné dans son téléphone se retrouvent dans la télé et il la place où il veut. À l'écran. Et quand il appuie sur un bouton, ça laisse l'image. Voilà donc il a fait deux trucs. Il a fait l'application qui permet de faire de systématiser l'extraction d'arrière plan et qui tourne sur mobile et une un client qui tourne sur un ordinateur et qui lui est capable de synchroniser avec le téléphone pour dire au téléphone Voilà si tu vois ça de de l'écran c'est-à-dire que ici en X Y et donc de pouvoir transférer l'image et c'est une manière de travailler. Je pense que c'est vraiment une utilisation. Très innovant e et à la fois des nouveaux devices de justement de tout ce qu'ils apportent de création dans la mobilité et et au niveau d'internet etc.

Ça met la barre assez haut par rapport au workflow traditionels en fait, puisque t'as quelque chose qui est très fluide en fait, entre la capture d'une image ou de plusieurs images. Ça donne vraiment une bonne idée de ce que peut être le travail d'un DA, avec ses outils, avec cette fluidité en fait. Il y a pas longtemps, ils ont mis à dispo un modèle qui fait in painting ça permet d'effacer quelque chose, une image. Donc ça Photoshop le fait depuis assez longtemps, je ne sais pas exactement comment mais là voilà, là l'idée c'est de le mettre à dispo de sur sur un site web tu vois, 

ce que j'aimerais bien voir plus moi c'est des modèles qui sont mis à dispo sur Internet et qui permet comme ça d'effectuer, mais ça rejoint plutôt ce que tu disais au début des finalement des petites tâches assez spécialisés mais accessibles sur un drag-drop dans une fenêtre dans une fenêtre de navigateur 

c'était juste que j'ai l'impression aussi dans la série des outils. Que les que les créatifs peuvent se mettre un peu plus facilement dans les mains. Il y a toutes les tous les Colab, les notebook Python de plus en plus à la sortie des papiers, ils mettent en place. ils mettent en place un collab qui permet de tester en fait le code avec ses propres images.

Donc ça, ça reste encore un petit peu. Un petit pas très intuitif mais je trouve que c'est un pas qui qui va dans la bonne direction de justement de vulgariser de rendre accessible. Parce que au fond, si tu prends le temps de lire les instructions et que tu arrives à brancher ton drive, ce qui est pas super compliqué et assez vite on peut se servir de tout. On peut aller comme ça tirer parti de finalement des vraiment des derniers papiers et juste finalement essayer quoi? 

**Alexia:** Ici donc, complètement, parce que justement Douglas nos enseignants semestre dernier, il a utilisé Google justement pour les étudiants et étudiantes puissent entraîner des petites créatures en ligne, en fait à partir de leur propre base de données.

Finalement, un designer, un créatif est ce qu'il a vraiment besoin de comprendre tout ce qui se passe derrière avec le les algorithmes à part on a mentionné Justement, ce qui serait bien, c'est peut être d'apprendre à faire ses propres bases de données, mais ce qu'il y aurait d'autres éléments qu'il faudrait qu'ils sentent vraiment important à comprendre ou au contraire, c'est pas si important que ça.

**Nicolas:** Il faudrait qu'il ait envie parce que je pense pas que en fait le fondamentalement c'est c'est quand même du gros code. C'est pas c'est pas lol du tout, et je me méfie comme de la peste des gens qui disent en cinquante lignes ou moins. Je sais pas quoi fait un truc en cinquante lignes c'est derrière il y a peut être cinquante mille peut être cinq mais sauf que derrière ces des pyramides de code qui sont utilisés voilà après, ça dépend parce que un un créatif comment comme on a pu dire jusqu'à il a finalement déjà pas mal d'outils qui se servent du machine learning qui exposent juste le haut niveau c'est-à-dire les fonctionnalités de haut niveau, s'est converti mon image style transfer

sélecionne l'image un et puis l'image deux appuie sur le bouton et ton donc ça c'est très bien en fait. Et je pense pas que dans un usage comme si je suis un peu bloqué sur Photoshop mais s'il y a un an, parce qu'après il y a sans doute des créatifs qui autre chose mais pour un usage type photoshop dans un dans un flux de production de type agence de marketing, il y aura jamais besoin de plus qu'un truc qui est bien emballé avec un bouton choisis ton image A et B et puis appuie sur le bouton et voilà Après c'est vraiment des outils qui sont qui sont qui sont très riches on va dire quand on quand on descend à la cave quoi? Parce que les les artistes alors pour le coup, c'est pas des créatifs successifs je fais une distinction des artistes qui vont aller, qui vont aller travailler si tu veux leur médium, c'est le, c'est le modèle, c'est le machine learning c'est l'outil, , la matière qui travaillent, c'est le modèle, les modèles. Je reprends mario klingemann ou Memo Akten c'est pas suffisant de d'ouvrir Runway et de chercher d'épuiser le modèle, le modèle qui a été livrée avec ça, ça, ça n'a aucun intérêt pour eux. Ce qui les intéresse, c'est d'aller justement descendre juste de comprendre comment . fonctionnent un modèle de optimisée, de le détourner, de le faire aller à des endroits où il n'est pas censé aller, de les combiner à des niveaux. Ça demande une compréhension du code qui fait tourner un modèle, qui sont qui sont folle, quoi qui sont qui? Sont vraiment très bas niveau justement par opposition au au niveau Style transfer où on me demande de choisir des images, d'appuyer sur un bouton et là il faut Il faut avoir vraiment une comprendre comment le modèle est structuré, comprendre qu'il est que c'est des couches successives d'appels de fonction.

Enfin du moins, vous n'êtes pas là pour décrire ce que c'est un modèle, mais que c'est des des couches et que et que à un moment tu peux, tu peux intercepté une couche peut inverser une couche, tu peux réinjecter une couche, peut injecter à certains moments certaines informations et et donc ça, ça demande une connaissance qui est vraiment très très profonde.

Et à mon avis donc les seuls créatifs qui font du machine learning, c'est ces gens-là, c'est parce que c'est c'est ceux qui ont qui utilisent le machine learning comme media ce qui ce qui ne fait pas de des autres créatif qui servent du machine learning des idiots. C'est juste que je pense que tout le monde n'est pas prêt à franchir le pas de justement cette compréhension profonde.

**Alexia:** , Je ne sais pas si t'as vu ces outils. Ça s'appelle Sketch to Code Et moi j'ai encore jamais essayé. Tu dessines en fait. et je pense qu'il y a un modèle qui transfère ton dessin en prototype HTML et du coup je je me demandais où ce type d'outil pouvait aller au final et puis Et si ça si ça reste vraiment juste à l'état de gadgets en fait, parce que même si tu as ton prototype en html tu dois savoir codé pour le 

**Nicolas:** pour le il y a la promesse et puis après il y a la réalité. Je vois très bien ce dont tu parles et ça m'avait tellement bluffé si tu veux un an ou deux après un mec carrément avait rajouté une couche en, donc en langue naturelle. Il parlait à son ordinateur, il disait Je veux un carré rouge à droite à gauche. Et donc c'est une première première passe qui était de de faire du speech to text. et ensuite une seconde qui était de de transformer ça en instruction HTML. Je pense que ça reste quand même très très gadget en En fait, c'est pas que la technologie n'est pas mature parce que si un prototype existe, ça veut dire que le reste c'est du travail, mais que c'est tellement niche par rapport à la demande. Il faudrait tellement tellement d'énergie, tellement de travail, probablement plusieurs années de développement pour faire un sketch to code qui ressemble à quelque chose qui soit prêt pour partir enfin pour être utilisé en production et en face de ça t'a une expertise de beaucoup de gens qui savent faire des pages html et qui les feront plus proprement et aussi vite en fait

d'ailleurs le développement HTML par exemple, c'est devenu plutôt une une industrie de type cols bleus. En fait, il y a beaucoup, c'est quasiment un travail non qualifié en fait et par parenthèse. Pour te dire à quel point c'est pas compliqué de faire un site web Et donc ces outils là, moi je pense que ça reste plutôt de l'ordre de la demo technique

Il y a oui qui fait relativement beaucoup de polémique parmi les dev c'est co-pilote de GitHub est donc un assistant qui fait de la compilation de code fait sauf que pour faire la compilation de code il a scanné tout GitHub et ça pose des problèmes parce qu'il te rechrache des morceaux quasiment texto, de de code que d'autres gens ont écrit, et non pas forcément donné leur accord pour être utilisé.

D'ailleurs, ça rejoint un peu les histoires biais en machine learning c'est très problématique, 

**Alexia:** Les questions de droits d'auteur, même quand on fait du webscrapping elles vont se poser. Et puis je ne sais pas si on alors je sais pas si on est si l'industrie est vraiment au fait de ces problématiques là.

**Nicolas:** C'est vraiment embarrassant ces modèles, Ces modèles probabilistes ou sont déjà utilisées. En vrai, c'est pour pour accorder un prêt. C'est pas c'est pas ton conseiller de la banque qui le fait. C'est un modèle, un modèle de machines. Quand tu une banque, une assurance ou quoi? Il y a des petite ligne qui disent est ce que vous voulez bien que on utilise les données pour et puis si tu si tu veux pas, t'as pas le droit de signer. Donc en fait c'est mignon de demander. Voilà, et c'est pour ça qu'il faut faire attention à ces à ces histoires de biais mais bon on est, on est graduellement. , on va aller voir de plus en plus partout. Enfin c'est ça, ça se fait un peu malgré nous 

**Alexia:** et donc peut être l'importance aussi de oui, de parler de peut-être, d'enseigner aussi le l'aspect plus critique aussi de ces et de ne pas rester forcément dans la dans le côté technique 

**Nicolas:** Ouais, il y avait un modèle qui fait aussi qui était un qui était la classe, des modèles qui font de la restauration d'image. Donc en gros tu prends une toute petite photo et il ressort une image. très grande rajoute du détail en fait il fait un peu le rêve là je sais pas comment s'appelle la série, mais c'est un truc policier où ils arrivent à zoomer super super loin dans un truc bref c'est un peu le le c'est un peu ça et et donc voilà, est ce truc-là?

Euh avait été pris assez littéralement et ça peut être effrayant. Comme un moyen dans une manifestation, de pouvoir retrouver des gens, même s'ils sont tout petits, sauf que dans ce processus, il y a une Il y a une telle perte d'information au moment de la compression de l'image pour que ça devienne une petite bouillie de pixel que en fait , c'est vraiment c'est une extrapolation. Il y a aucun moyen que tu retrouves la même personne sauf si la personne a un visage extrêmement générique et que voilà que c'est ça qui a été reconstruit.

En parallèle des modèles qui sont discriminants et voilà, ils sont déjà en place. Je reprends mon histoire de banque, mais voilà. Parce que que tu as tel revenu tu tombes dans telle catégorie, en fait, dans telle classe de clients et donc de fait, on va mécaniquement bloquer certains trucs.

**Alexia:** Du coup, pour conclure notre notre entretien est-ce que toi le machine learning, on parle de machine learning et pas d'IA, est ce que toi ça t'effraie d'une manière ou d'une autre? On voit souvent ce dialogue un peu caricaturale de voilà attention les graphic designer l'IA va vous remplacer c'est un débat qui est qui est qui est presque enfin complètement caricaturale.

**Nicolas:** On aime se faire peur et qu'on donne beaucoup plus de pouvoir que qu'elle a en fait déjà souvent un bon moyen de savoir à qui tu parles. C'est de ça de d'écouter s'ils utilisent IA ou machine learning que il y a une blagque que j'aime beaucoup, c'est le machine learning ça s'écrit avec du python et l'IA ça s'écrit avec du powerpoint

Quand on sait de quoi il retourne on a peut être un peu moins peur sauf des applications concrètes dans la vie de tous les jours et concrète et incontrôlée. Ou alors où où le consentement est forcé, 

la participation est forcément un peu enrôlés malgré toi. Autre utilisation pourries tout ce qui est Facebook et companie, les GAFAM parce que je veux dire que pour entraîner et je pense qu'il doit avoir un modèle par l'utilisateur un peu près et qui permet ensuite de refourger différente pub parce que c'est comme cela qu'ils vivent donc ça c'est des vrai Mécaniques qui sont qui sont problématiques. Il y a le donc les biais dans le machine learning quelques initiatives, notamment pour le genre. Je parlais de Style Gan plein de fois là, mais d'ailleurs je fais une série là-dessus et en gros le je l'ai fait tourner avec les paramètres par défaut.

et sur trois mille trois mille images qu'il a généré, il y en avait à dire quatre-vingts qui m'ont frappé comme étant pas caucasiens ou asiatique d'ailleurs puisque les deux les deux sont beaucoup beaucoup représentée et qui étaient en gros noir ou arabe et et ça m'a ça m'a fait chier donc après je suis allé me renseigner un petit peu plus et il y avait déjà des universitaires qui avait un petit peu soulevé le problème en disant parce qu'un modèle, mais finalement c'est le résultat de ce avec quoi il a été entraîné et en l'occurrence ce truc-là étaient faits avaient été entraînés sur des images Flickr donc plusieurs plusieurs centaines de millions d'images. Flickr et la majorité des des gens qui étaient représentés sur les images étaient donc plutôt des blancs caucasiens mais ils n'ont pas du tout fait gaffe à ce à ce qui fait que le modèle il est complètement biaisé.

Il faut il faut faire gaffe mais comme comme c'est ceux qui ont la main sur la création des modèles qui sont en charge de ça Donc ça, ça vient, je pense qu'il y a eu deux trois.

Maintenant les gens font un peu plus attention. Mais , mais ça il faut. Enfin oui, il y a un truc qui m'effraie avec le machine learning et enfin c'est des choses qui sont insidieux et qui sont assez et qui sont qui sont aussi impossible à vérifier parce que généralement ils donnent pas leur data set, etc. Pourquoi maintenant si c'est un peu plus pour pour pouvoir justement mettre comparer entre différents modèles sur lesquels ils ont entraîné leurs trucs, au moins ça, et puis avoir un peu les mêmes, avoir les mêmes bases, on peut évaluer les résultats, donc ça vient. , je suis pas vraiment inquiet sur le côté ça va tuer des créatifs ça va plus les créatifs, par contre ça, ça peut je suis curieux de voir dans quelle mesure par exemple des animateurs des travaux qui sont très répétitifs bon pouvoir s'affranchir de certaines tâches qui sont juste répétitive.

Quoique si on peut dire que l'inter comme on en animation état des des et puis en fait tu fais des trucs l'interpellation entre les les images c'est pas forcément je sais pas si les modèles sont bons. suffisamment bon mais ça pourrait voilà quoi ça pourrait leur donner plus de temps pour faire des choses qui sont plus intéressants.

Donc améliorer la productivité sans perte de qualité puis la fin, les émanciper si tu veux dire. Mais quant à faire une créa automatiquement, ce sera toujours de la boucherie quoi. Enfin, il y a Clipe maintenant qui arrive à faire des choses pas trop mal, mais ça reste très ça reste presque grotesque. Tu vois comme c'est un ordinateur qui clip, c'est un truc qui a été un entraîné sur une image et sa description textuelle.

Et donc du coup, on peut en tu peux créer à partir d'une description textuelle, une image et c'est intéressant. C'est il y a beaucoup de gens qui travaillent avec ce truc là en ce moment, c'est c'est vraiment très riche. Ça permet de faire une classification aussi, qui est assez intelligente des images quand tu as à des collections d'images par exemple, ce qu'ils font, ce qu'ils font chez Google en ce moment, c'est les collections d'images.

Et tu veux pouvoir chercher? En en tapant des textes libres. Du coup ça va être amené des résultats qui sont assez pertinent. Donc ça c'est de ouais, mais c'est pas c'est pas quelque chose qui qui ça ne m'effraie pas Parce qu'en fait tant que tant qu'une IA, il ne sera pas capable de s'assumer tout seul.

C'est bon quoi mais

**Alexia:** non merci beaucoup. Merci beaucoup Nicolas
